{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy \n",
    "import numpy as np\n",
    "from scipy.special import erfinv\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import fsolve\n",
    "from scipy.optimize import bisect\n",
    "import numpy.random as rng\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from copy import deepcopy \n",
    "from tqdm import tqdm\n",
    "import time\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Related Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator_doubleNormal(object):\n",
    "    def __init__(self, p, theta, sigma0, sigma1, sigma=1) -> None:\n",
    "        self.p = p \n",
    "        self.theta = theta \n",
    "        self.sigma0 = sigma0\n",
    "        self.sigma1 = sigma1 \n",
    "        self.sigma = sigma\n",
    "        self.X = np.eye(p)\n",
    "    \n",
    "    def generate_samples(self, n):\n",
    "        theta = np.ones((n, self.p)) * self.theta\n",
    "        gamma = rng.binomial(1, theta)\n",
    "        beta = np.zeros((n, self.p))\n",
    "        beta[gamma == 1] = rng.randn(np.sum(gamma == 1)) * self.sigma1\n",
    "        beta[gamma == 0] = rng.randn(np.sum(gamma == 0)) * self.sigma0 \n",
    "        Y = beta@self.X.T + rng.randn(n, self.p) * self.sigma \n",
    "        return gamma, beta, Y "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution related parameters are defined below. Training set are used to estimate the mean and standard deviation of Y, which are used to normalize the input to the Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 0.05\n",
    "sigma0 = 0.1\n",
    "sigma1 = 5\n",
    "sigma = 1\n",
    "lr = 0.001 # learning rate\n",
    "p = 50\n",
    "generator = Generator_doubleNormal(p, theta, sigma0, sigma1)\n",
    "\n",
    "rng.seed(0)\n",
    "gamma_train, beta_train, Y_train = generator.generate_samples(1000000)\n",
    "gamma_val, beta_val, Y_val = generator.generate_samples(10000)\n",
    "\n",
    "rng.seed(1)\n",
    "gamma_test, beta_test, Y_test = generator.generate_samples(1000000)\n",
    "\n",
    "mean = Y_train.mean(0)\n",
    "std = Y_train.std(0)\n",
    "\n",
    "val_dataset = TensorDataset(torch.Tensor((Y_val - mean) / std), torch.Tensor(beta_val))\n",
    "valid_dataloader = DataLoader(val_dataset, batch_size=len(val_dataset))\n",
    "\n",
    "Y_test_normalized = (Y_test - mean) / std"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True Posterior Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The posterior is also a Gaussian mixture of two components. The posterior mean is easy to compute. However, the \n",
    "quantile doesn't have close form solution. Use fsolve in scipy.optimize to solve equations for quantile.\n",
    "'''\n",
    "def computeNormalStats(sigma, sigma0, sigma1, Y):\n",
    "    tau = 1 / sigma**2\n",
    "    tau0 = 1 / sigma0**2\n",
    "    tau1 = 1 / sigma1**2\n",
    "    tau0_ = tau0 + tau \n",
    "    tau1_ = tau1 + tau \n",
    "    theta_ = theta / (theta + (1-theta) * np.sqrt(tau1_*tau0/(tau0_*tau1)) \\\n",
    "        * np.exp(-0.5 * tau**2 * (tau0_-tau1_) / (tau0_*tau1_) * Y**2))\n",
    "    mu1 = tau / tau1_ * Y \n",
    "    mu0 = tau / tau0_ * Y \n",
    "    true_mean = theta_ * mu1 + (1-theta_) * mu0\n",
    "    # Use different initial values according to Y to avoid solver failure.\n",
    "    if np.abs(Y) > 4:\n",
    "        q025 = fsolve(lambda x: theta_ * norm.cdf(x, mu1, (1/tau1_)**0.5) + (1-theta_) * norm.cdf(x, mu0, (1/tau0_)**0.5) - 0.025, Y)\n",
    "        q975 = fsolve(lambda x: theta_ * norm.cdf(x, mu1, (1/tau1_)**0.5) + (1-theta_) * norm.cdf(x, mu0, (1/tau0_)**0.5) - 0.975, Y)\n",
    "    else:\n",
    "        q025 = fsolve(lambda x: theta_ * norm.cdf(x, mu1, (1/tau1_)**0.5) + (1-theta_) * norm.cdf(x, mu0, (1/tau0_)**0.5) - 0.025, 0)\n",
    "        q975 = fsolve(lambda x: theta_ * norm.cdf(x, mu1, (1/tau1_)**0.5) + (1-theta_) * norm.cdf(x, mu0, (1/tau0_)**0.5) - 0.975, 0)        \n",
    "    return true_mean, q025, q975"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Related Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_variant(nn.Module):\n",
    "    def __init__(self, N, p, num_nodes, ac_func='relu', dropout=0):\n",
    "        super(MLP_variant, self).__init__()\n",
    "        fc = []\n",
    "        last_node = N\n",
    "        for node in num_nodes:\n",
    "            fc.append(nn.Linear(last_node, node))\n",
    "            last_node = node \n",
    "        self.fc_final = nn.Linear(last_node, p)\n",
    "        self.fc = nn.ModuleList(fc)\n",
    "        assert ac_func in ('relu', 'tanh', 'leakyrelu', 'softplus', 'softsign', 'selu', 'elu')\n",
    "        if ac_func == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif ac_func == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif ac_func == 'leakyrelu':\n",
    "            self.activation = nn.LeakyReLU()\n",
    "        elif ac_func == 'softplus':\n",
    "            self.activation = nn.Softplus()\n",
    "        elif ac_func == 'softsign':\n",
    "            self.activation = nn.Softsign()\n",
    "        elif ac_func == 'selu':\n",
    "            self.activation = nn.SELU()\n",
    "        elif ac_func == 'elu':\n",
    "            self.activation = nn.ELU()\n",
    "        self.mseloss = nn.MSELoss()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for m in self.fc:\n",
    "            x = self.dropout(self.activation(m(x)))\n",
    "        x = self.fc_final(x)\n",
    "        return x \n",
    "    \n",
    "    def get_mseloss(self, data, targ):\n",
    "        output = self.forward(data)\n",
    "        loss = self.mseloss(output, targ)\n",
    "        return loss \n",
    "\n",
    "    def get_quanloss(self, data, targ, tau):\n",
    "        output = self.forward(data)\n",
    "        errs = targ - output \n",
    "        loss = torch.mean(torch.max((tau-1)*errs, tau*errs))\n",
    "        return loss         "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(model, data_loader, loss_type='mse', q=0.5, kwargs=None):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        n = 0 \n",
    "        total_loss = 0.\n",
    "        for _, (data, targ) in enumerate(data_loader):\n",
    "            data, targ = data.to(device), targ.to(device)\n",
    "            if kwargs:\n",
    "                if 'subset' in kwargs:\n",
    "                    targ = targ[:,(kwargs['subset'][0]-1):kwargs['subset'][1]]\n",
    "            if loss_type == 'mse':\n",
    "                loss = model.get_mseloss(data, targ)\n",
    "            elif loss_type == 'bce':\n",
    "                loss = model.get_bceloss(data, targ)\n",
    "            elif loss_type == 'quantile':\n",
    "                loss = model.get_quanloss(data, targ, q)\n",
    "            total_loss += loss.item() * data.shape[0]\n",
    "            n += data.shape[0]\n",
    "    return total_loss/n\n",
    "\n",
    "def predict(model, Y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        data = torch.from_numpy(Y).type(torch.float).to(device)\n",
    "        pred = model(data)\n",
    "    return pred.detach().cpu().numpy()\n",
    "\n",
    "def show_loss(train_losses, val_losses):\n",
    "    plt.plot(range(len(train_losses)), train_losses)\n",
    "    plt.plot(range(len(train_losses)), val_losses)\n",
    "    plt.legend(['train loss', 'val loss'], loc=\"upper right\")\n",
    "    plt.show()\n",
    "\n",
    "def predict_class(model, Y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        data = torch.from_numpy(Y).type(torch.float).to(device)\n",
    "        pred = torch.sigmoid(model(data))\n",
    "    return pred.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The following training function uses fixed training data.\n",
    "'''\n",
    "''' \n",
    "loss_type: 'mse' for posterior mean, 'bce' for predicting whether beta is 0, 'quantile' for posterior quantile.\n",
    "q: Only used when loss_type is 'quantile', q quantile.\n",
    "'''\n",
    "def train_epoch(model, optimizer, data_loader, loss_type, q, kwargs):\n",
    "    model.train()\n",
    "    n = 0\n",
    "    train_loss = 0.\n",
    "    for _, (data, targ) in enumerate(data_loader):\n",
    "        data, targ = data.to(device), targ.to(device)\n",
    "        if loss_type == 'mse':\n",
    "            loss = model.get_mseloss(data, targ)\n",
    "        elif loss_type == 'bce':\n",
    "            loss = model.get_bceloss(data, targ)\n",
    "        elif loss_type == 'quantile':\n",
    "            loss = model.get_quanloss(data, targ, q)\n",
    "        train_loss += loss.item() * data.shape[0]   \n",
    "        n += data.shape[0]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if 'scheduler' in kwargs:\n",
    "        kwargs['scheduler'].step()\n",
    "    return train_loss/n\n",
    "             \n",
    "def train_model(model, model_es, optimizer, epochs, train_data, loss_type='mse', q=0.5, val_data=None, early_stop=10, **kwargs):\n",
    "    assert loss_type in ['mse', 'bce', 'quantile']\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    min_loss = 1e6\n",
    "    es_count = 0\n",
    "    es_flag = 0\n",
    "    for i in range(epochs):\n",
    "        train_loss = train_epoch(model, optimizer, train_data, loss_type, q, kwargs)\n",
    "        print('Epoch: {}'.format(i+1))\n",
    "        print('Train loss: {:.5f}'.format(train_loss))\n",
    "        train_losses.append(train_loss)\n",
    "        if val_data.__str__() != 'None':\n",
    "            val_loss = model_test(model, val_data, loss_type, q)\n",
    "            print('Val loss: {:.5f}'.format(val_loss))\n",
    "            val_losses.append(val_loss)\n",
    "            if val_loss <= min_loss:\n",
    "                min_loss = val_loss\n",
    "                es_count = 0\n",
    "            if (es_count >= early_stop) and (es_flag == 0):\n",
    "                es_flag = 1\n",
    "                print('Save early stopping model at epoch {}'.format(i+1))\n",
    "                model_es.load_state_dict(deepcopy(model.state_dict()))\n",
    "            es_count += 1\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The following training function uses brand new samples at each batch.\n",
    "'''\n",
    "def train_epoch_with_generator(model, optimizer, generator, batch_size, iteration, loss_type, q, kwargs):\n",
    "    model.train()\n",
    "    train_loss = 0.\n",
    "    for i in range(iteration):\n",
    "        gamma, beta, Y = generator.generate_samples(batch_size)\n",
    "        Y = (Y - kwargs['mean']) / kwargs['std']\n",
    "\n",
    "        if 'subset' in kwargs:\n",
    "            gamma = torch.from_numpy(gamma[:,(kwargs['subset'][0]-1):kwargs['subset'][1]]).type(torch.float).to(device)\n",
    "            beta = torch.from_numpy(beta[:,(kwargs['subset'][0]-1):kwargs['subset'][1]]).type(torch.float).to(device)\n",
    "            Y = torch.from_numpy(Y).type(torch.float).to(device)\n",
    "        else:\n",
    "            gamma = torch.from_numpy(gamma).type(torch.float).to(device)\n",
    "            beta = torch.from_numpy(beta).type(torch.float).to(device)\n",
    "            Y = torch.from_numpy(Y).type(torch.float).to(device)\n",
    "\n",
    "        if loss_type == 'mse':\n",
    "            loss = model.get_mseloss(Y, beta)\n",
    "        elif loss_type == 'bce':\n",
    "            loss = model.get_bceloss(Y, gamma)\n",
    "        elif loss_type == 'quantile':\n",
    "            loss = model.get_quanloss(Y, beta, q)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if 'scheduler' in kwargs:\n",
    "        kwargs['scheduler'].step()\n",
    "    return train_loss/(i+1)\n",
    "\n",
    "# Input mean and std to normalize input.\n",
    "# Input subset to take a subset of coordinates.\n",
    "def train_model_with_generator(model, generator, optimizer, epochs, batch_size, iteration_per_epoch, loss_type='mse', q=0.5, val_data=None, **kwargs):\n",
    "    assert loss_type in ['mse', 'bce', 'quantile']\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for i in range(epochs):\n",
    "        train_loss = train_epoch_with_generator(\n",
    "            model, optimizer, generator, batch_size, iteration_per_epoch, loss_type, q, kwargs)\n",
    "        print('Epoch: {}'.format(i+1))\n",
    "        print('Train loss: {:.5f}'.format(train_loss))\n",
    "        train_losses.append(train_loss)\n",
    "        if val_data.__str__() != 'None':\n",
    "            val_loss = model_test(model, val_data, loss_type, q, kwargs)\n",
    "            print('Val loss: {:.5f}'.format(val_loss))\n",
    "            val_losses.append(val_loss)\n",
    "        if 'model_list' in kwargs:\n",
    "            if (i+1) in kwargs['save_point']:\n",
    "                kwargs['model_list'].append(deepcopy(model.state_dict()))\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gold Standard Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Only the first 10,000 test samples are used for comparison. \n",
    "'''\n",
    "true_mean = np.zeros((10000, p))\n",
    "true_q025 = np.zeros((10000, p))\n",
    "true_q975 = np.zeros((10000, p))\n",
    "\n",
    "for i in tqdm(range(10000)):\n",
    "    for j in range(p):\n",
    "        true_mean[i,j], true_q025[i,j], true_q975[i,j] = computeNormalStats(sigma, sigma0, sigma1, Y_test[i,j])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Model (output dimension = 50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP_variant(input dimension, output dimension, hidden units, activation function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_q025 = MLP_variant(p, p, [1024, 1024], 'leakyrelu').to(device) \n",
    "optimizer = torch.optim.Adam(md_q025.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20, 30, 40, 50, 60, 70, 80], gamma=0.4)\n",
    "train_losses, val_losses = train_model_with_generator(md_q025, generator, optimizer, epochs=100, \n",
    "                                      batch_size=256, iteration_per_epoch=4000, loss_type='quantile',\n",
    "                                      q=0.025, val_data=valid_dataloader, scheduler=scheduler, mean=mean, std=std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_loss(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_q975 = MLP_variant(p, p, [1024, 1024], 'leakyrelu').to(device)\n",
    "optimizer = torch.optim.Adam(md_q975.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20, 30, 40, 50, 60, 70, 80], gamma=0.4)\n",
    "train_losses, val_losses = train_model_with_generator(md_q975, generator, optimizer, epochs=100, \n",
    "                                      batch_size=256, iteration_per_epoch=4000, loss_type='quantile',\n",
    "                                      q=0.975, val_data=valid_dataloader, scheduler=scheduler, mean=mean, std=std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_loss(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_mean = MLP_variant(p, p, [1024, 1024], 'leakyrelu').to(device)\n",
    "optimizer = torch.optim.Adam(md_mean.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20, 30, 40, 50, 60, 70, 80], gamma=0.4)\n",
    "train_losses, val_losses = train_model_with_generator(md_mean, generator, optimizer, epochs=100, \n",
    "                                      batch_size=256, iteration_per_epoch=4000, loss_type='mse',\n",
    "                                      val_data=valid_dataloader, scheduler=scheduler, mean=mean, std=std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_loss(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_q025 = predict(md_q025, Y_test_normalized)\n",
    "pred_q975 = predict(md_q975, Y_test_normalized)\n",
    "pred_mean = predict(md_mean, Y_test_normalized)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show comparison between true posterior and predicted of randomly chosen test samples (has non-zero beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = np.where(np.abs(beta_test[:10000,:])>2)[0]\n",
    "t = 1000 * time.time()\n",
    "np.random.seed(int(t) % 2**32)\n",
    "rng.shuffle(subset)\n",
    "subset = subset[:6]    \n",
    "\n",
    "plt.figure(figsize=(18,15))\n",
    "for i, idx in enumerate(subset):\n",
    "    plt.subplot(3,2,i+1)\n",
    "    plt.plot(range(p), true_mean[idx], '.b', ms=5)\n",
    "    plt.plot(np.arange(p)+0.2, pred_mean[idx], '.r', ms=5)\n",
    "    plt.vlines(np.arange(p), true_q025[idx], true_q975[idx], color='red', alpha=0.5, lw=2)\n",
    "    plt.vlines(np.arange(p)+0.2, pred_q025[idx], pred_q975[idx], color='green', alpha=0.5, lw=2)\n",
    "    plt.legend(['True mean', 'Pred mean', 'True CI', 'Pred CI'], ncol=2)\n",
    "    plt.ylim(-16,16)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE and Loss Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE between true posterior quantiles and predicted quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_q025 = np.mean((true_q025 - pred_q025[:10000,:])**2, 0)\n",
    "mse_q975 = np.mean((true_q975 - pred_q975[:10000,:])**2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mse_q025,'.')\n",
    "plt.xlabel('coordinate')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('q025 coordinate wise mse')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mse_q975,'.')\n",
    "plt.xlabel('coordinate')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('q975 coordinate wise mse')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Compute test set loss\n",
    "'''\n",
    "q025_loss = np.mean(np.maximum(0.025*(beta_test-pred_q025),0.975*(pred_q025-beta_test)), 0)\n",
    "q975_loss = np.mean(np.maximum(0.975*(beta_test-pred_q975),0.025*(pred_q975-beta_test)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.where(mse_q025<0.1)[0].reshape(-1), q025_loss[np.where(mse_q025<0.1)], 'o')\n",
    "plt.plot(np.where(mse_q025>=0.1)[0].reshape(-1), q025_loss[np.where(mse_q025>=0.1)], 'o')\n",
    "plt.legend(['mse<0.1', 'mse>=0.1'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('coordinate')\n",
    "plt.title('q025')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.where(mse_q975<0.1)[0].reshape(-1), q975_loss[np.where(mse_q975<0.1)], 'o')\n",
    "plt.plot(np.where(mse_q975>=0.1)[0].reshape(-1), q975_loss[np.where(mse_q975>=0.1)], 'o')\n",
    "plt.legend(['mse<0.1', 'mse>=0.1'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('coordinate')\n",
    "plt.title('q975')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last layers' bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paras_q025 = []\n",
    "for name, para in md_q025.named_parameters():\n",
    "    print(name)\n",
    "    paras_q025.append(copy.deepcopy(para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q025_final_bias = paras_q025[1].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.where(mse_q025<0.1)[0].reshape(-1), q025_final_bias[np.where(mse_q025<0.1)], 'o')\n",
    "plt.plot(np.where(mse_q025>=0.1)[0].reshape(-1), q025_final_bias[np.where(mse_q025>=0.1)], 'o')\n",
    "plt.legend(['mse<0.1', 'mse>=0.1'])\n",
    "plt.ylabel('bias')\n",
    "plt.xlabel('coordinate')\n",
    "plt.title('q025 last layer bias')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paras_q975 = []\n",
    "for name, para in md_q975.named_parameters():\n",
    "    print(name)\n",
    "    paras_q975.append(copy.deepcopy(para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q975_final_bias = paras_q975[1].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.where(mse_q975<0.1)[0].reshape(-1), q975_final_bias[np.where(mse_q975<0.1)], 'o')\n",
    "plt.plot(np.where(mse_q975>=0.1)[0].reshape(-1), q975_final_bias[np.where(mse_q975>=0.1)], 'o')\n",
    "plt.legend(['mse<0.1', 'mse>=0.1'])\n",
    "plt.ylabel('bias')\n",
    "plt.xlabel('coordinate')\n",
    "plt.title('q975 last layer bias')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = (1, 20)\n",
    "md_q025_1_20 = MLP_variant(p, 20, [1024, 1024], 'leakyrelu').to(device)\n",
    "optimizer = torch.optim.Adam(md_q025_1_20.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20, 30, 40, 50, 60, 70, 80], gamma=0.4)\n",
    "train_losses, val_losses = train_model_with_generator(md_q025_1_20, generator, optimizer, epochs=100, \n",
    "                                      batch_size=256, iteration_per_epoch=4000, loss_type='quantile',\n",
    "                                      q=0.025, val_data=valid_dataloader, scheduler=scheduler,\n",
    "                                      mean=mean, std=std, subset=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_loss(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_q975_1_20 = MLP_variant(p, 20, [1024, 1024], 'leakyrelu').to(device)\n",
    "optimizer = torch.optim.Adam(md_q975_1_20.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20, 30, 40, 50, 60, 70, 80], gamma=0.4)\n",
    "train_losses, val_losses = train_model_with_generator(md_q975_1_20, generator, optimizer, epochs=100, \n",
    "                                      batch_size=256, iteration_per_epoch=4000, loss_type='quantile',\n",
    "                                      q=0.975, val_data=valid_dataloader, scheduler=scheduler,\n",
    "                                      mean=mean, std=std, subset=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_loss(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = (21, 40)\n",
    "md_q025_21_40 = MLP_variant(p, 20, [1024, 1024], 'leakyrelu').to(device)\n",
    "optimizer = torch.optim.Adam(md_q025_21_40.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20, 30, 40, 50, 60, 70, 80], gamma=0.4)\n",
    "train_losses, val_losses = train_model_with_generator(md_q025_21_40, generator, optimizer, epochs=100, \n",
    "                                      batch_size=256, iteration_per_epoch=4000, loss_type='quantile',\n",
    "                                      q=0.025, val_data=valid_dataloader, scheduler=scheduler,\n",
    "                                      mean=mean, std=std, subset=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_loss(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_q975_21_40 = MLP_variant(p, 20, [1024, 1024], 'leakyrelu').to(device)\n",
    "optimizer = torch.optim.Adam(md_q975_21_40.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20, 30, 40, 50, 60, 70, 80], gamma=0.4)\n",
    "train_losses, val_losses = train_model_with_generator(md_q975_21_40, generator, optimizer, epochs=100, \n",
    "                                      batch_size=256, iteration_per_epoch=4000, loss_type='quantile',\n",
    "                                      q=0.975, val_data=valid_dataloader, scheduler=scheduler,\n",
    "                                      mean=mean, std=std, subset=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_loss(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = (41, 50)\n",
    "md_q025_41_50 = MLP_variant(p, 10, [1024, 1024], 'leakyrelu').to(device)\n",
    "optimizer = torch.optim.Adam(md_q025_41_50.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20, 30, 40, 50, 60, 70, 80], gamma=0.4)\n",
    "train_losses, val_losses = train_model_with_generator(md_q025_41_50, generator, optimizer, epochs=100, \n",
    "                                      batch_size=256, iteration_per_epoch=4000, loss_type='quantile',\n",
    "                                      q=0.025, val_data=valid_dataloader, scheduler=scheduler,\n",
    "                                      mean=mean, std=std, subset=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_loss(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_q975_41_50 = MLP_variant(p, 10, [1024, 1024], 'leakyrelu').to(device)\n",
    "optimizer = torch.optim.Adam(md_q975_41_50.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20, 30, 40, 50, 60, 70, 80], gamma=0.4)\n",
    "train_losses, val_losses = train_model_with_generator(md_q975_41_50, generator, optimizer, epochs=100, \n",
    "                                      batch_size=256, iteration_per_epoch=4000, loss_type='quantile',\n",
    "                                      q=0.975, val_data=valid_dataloader, scheduler=scheduler,\n",
    "                                      mean=mean, std=std, subset=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_loss(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_q025_1_20 = predict(md_q025_1_20, Y_test_normalized)\n",
    "pred_q975_1_20 = predict(md_q975_1_20, Y_test_normalized)\n",
    "pred_q025_21_40 = predict(md_q025_21_40, Y_test_normalized)\n",
    "pred_q975_21_40 = predict(md_q975_21_40, Y_test_normalized)\n",
    "pred_q025_41_50 = predict(md_q025_41_50, Y_test_normalized)\n",
    "pred_q975_41_50 = predict(md_q975_41_50, Y_test_normalized)\n",
    "pred_q025_c = np.c_[pred_q025_1_20, pred_q025_21_40, pred_q025_41_50]\n",
    "pred_q975_c = np.c_[pred_q975_1_20, pred_q975_21_40, pred_q975_41_50]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posterior mean predicted using previous mean model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mean = predict(md_mean, Y_test_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_q025_c = np.mean((true_q025 - pred_q025_c[:10000,:])**2, 0)\n",
    "mse_q975_c = np.mean((true_q975 - pred_q975_c[:10000,:])**2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mse_q025_c, '.')\n",
    "plt.ylabel('mse') \n",
    "plt.xlabel('coordinate')\n",
    "plt.title('q025')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mse_q975_c, '.')\n",
    "plt.ylabel('mse') \n",
    "plt.xlabel('coordinate')\n",
    "plt.title('q975')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = np.where(np.abs(beta_test[:10000,:])>2)[0]\n",
    "t = 1000 * time.time()\n",
    "np.random.seed(int(t) % 2**32)\n",
    "rng.shuffle(subset)\n",
    "subset = subset[:6]    \n",
    "\n",
    "plt.figure(figsize=(18,15))\n",
    "for i, idx in enumerate(subset):\n",
    "    plt.subplot(3,2,i+1)\n",
    "    plt.plot(range(p), true_mean[idx], '.b', ms=5)\n",
    "    plt.plot(np.arange(p)+0.2, pred_mean[idx], '.r', ms=5)\n",
    "    plt.vlines(np.arange(p), true_q025[idx], true_q975[idx], color='red', alpha=0.5, lw=2)\n",
    "    plt.vlines(np.arange(p)+0.2, pred_q025_c[idx], pred_q975_c[idx], color='green', alpha=0.5, lw=2)\n",
    "    plt.legend(['True mean', 'Pred mean', 'True CI', 'Pred CI'], ncol=2)\n",
    "    plt.ylim(-16,16)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output only the quantile of $\\beta_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = (1, 1)\n",
    "md_q025_1 = MLP_variant(p, 1, [1024, 1024], 'leakyrelu').to(device)\n",
    "optimizer = torch.optim.Adam(md_q025_1.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20, 30, 40, 50, 60, 70, 80], gamma=0.4)\n",
    "train_losses, val_losses = train_model_with_generator(md_q025_1, generator, optimizer, epochs=100, \n",
    "                                      batch_size=256, iteration_per_epoch=4000, loss_type='quantile',\n",
    "                                      q=0.025, val_data=valid_dataloader, scheduler=scheduler,\n",
    "                                      mean=mean, std=std, subset=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_loss(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_q975_1 = MLP_variant(p, 1, [1024, 1024], 'leakyrelu').to(device)\n",
    "optimizer = torch.optim.Adam(md_q975_1.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20, 30, 40, 50, 60, 70, 80], gamma=0.4)\n",
    "train_losses, val_losses = train_model_with_generator(md_q975_1, generator, optimizer, epochs=100, \n",
    "                                      batch_size=256, iteration_per_epoch=4000, loss_type='quantile',\n",
    "                                      q=0.975, val_data=valid_dataloader, scheduler=scheduler,\n",
    "                                      mean=mean, std=std, subset=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_loss(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_q025_1 = predict(md_q025_1, Y_test_normalized)\n",
    "pred_q975_1 = predict(md_q975_1, Y_test_normalized)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall MSE of 1-dimensional output model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('q025: ', np.mean((true_q025[:,:1] - pred_q025_1[:10000,:])**2))\n",
    "print('q975: ', np.mean((true_q975[:,:1] - pred_q975_1[:10000,:])**2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "316998951c33e0ee3e0273da6bff9fbd12eb355acb0b0db849e965619b9a3872"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
