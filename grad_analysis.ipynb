{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\97255\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rng\n",
    "import torch \n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from copy import deepcopy \n",
    "from model import MLP_variant\n",
    "from simulators import Generator_doubleNormal\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = MLP_variant(200, 200, [1024, 1024], ac_func='leakyrelu').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc_final.weight torch.Size([200, 1024])\n",
      "fc_final.bias torch.Size([200])\n",
      "fc.0.weight torch.Size([1024, 200])\n",
      "fc.0.bias torch.Size([1024])\n",
      "fc.1.weight torch.Size([1024, 1024])\n",
      "fc.1.bias torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "for n,p in md.named_parameters():\n",
    "    print(n, p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grads = {}\n",
    "for n, p in md.named_parameters():\n",
    "    train_grads[n] = []\n",
    "def normalize_constant_est(generator, n=100000, seed=0):\n",
    "    gamma_train, beta_train, Y_train = generator.generate_samples(n)\n",
    "    mean = Y_train.mean(0)\n",
    "    std = Y_train.std(0)\n",
    "    return mean, std \n",
    "\n",
    "def model_test(model, data_loader, loss_type='mse', q=0.5, kwargs=None):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        n = 0 \n",
    "        total_loss = 0.\n",
    "        for _, (data, targ) in enumerate(data_loader):\n",
    "            data, targ = data.to(device), targ.to(device)\n",
    "            if kwargs:\n",
    "                if 'subset' in kwargs:\n",
    "                    targ = targ[:,(kwargs['subset'][0]-1):kwargs['subset'][1]]\n",
    "            if loss_type == 'mse':\n",
    "                loss = model.get_mseloss(data, targ)\n",
    "            elif loss_type == 'bce':\n",
    "                loss = model.get_bceloss(data, targ)\n",
    "            elif loss_type == 'quantile':\n",
    "                loss = model.get_quanloss(data, targ, q)\n",
    "            elif loss_type == 'max_quantile':\n",
    "                loss = model.get_max_quanloss(data, targ, q)\n",
    "            total_loss += loss.item() * data.shape[0]\n",
    "            n += data.shape[0]\n",
    "    return total_loss/n\n",
    "\n",
    "def predict(model, Y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        data = torch.from_numpy(Y).type(torch.float).to(device)\n",
    "        pred = model(data)\n",
    "    return pred.detach().cpu().numpy()\n",
    "\n",
    "'''\n",
    "The following training function uses brand new samples at each batch.\n",
    "'''\n",
    "def train_epoch_with_generator(model, optimizer, generator, batch_size, iteration, loss_type, q, kwargs):\n",
    "    model.train()\n",
    "    train_loss = 0.\n",
    "    for i in range(iteration):\n",
    "        gamma, beta, Y = generator.generate_samples(batch_size)\n",
    "        Y = (Y - kwargs['mean']) / kwargs['std']\n",
    "        # print(np.sum(Y, 1)[0])\n",
    "\n",
    "        if 'subset' in kwargs:\n",
    "            gamma = torch.from_numpy(gamma[:,(kwargs['subset'][0]-1):kwargs['subset'][1]]).type(torch.float).to(device)\n",
    "            beta = torch.from_numpy(beta[:,(kwargs['subset'][0]-1):kwargs['subset'][1]]).type(torch.float).to(device)\n",
    "            Y = torch.from_numpy(Y).type(torch.float).to(device)\n",
    "        else:\n",
    "            gamma = torch.from_numpy(gamma).type(torch.float).to(device)\n",
    "            beta = torch.from_numpy(beta).type(torch.float).to(device)\n",
    "            Y = torch.from_numpy(Y).type(torch.float).to(device)\n",
    "\n",
    "        if loss_type == 'mse':\n",
    "            loss = model.get_mseloss(Y, beta)\n",
    "        elif loss_type == 'bce':\n",
    "            loss = model.get_bceloss(Y, gamma)\n",
    "        elif loss_type == 'quantile':\n",
    "            loss = model.get_quanloss(Y, beta, q)\n",
    "            for n, p in model.named_parameters():\n",
    "                train_grads[n].append(p.grad.data.cpu().numpy())\n",
    "        elif loss_type == 'max_quantile':\n",
    "            loss = model.get_max_quanloss(Y, beta, q)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if 'scheduler' in kwargs:\n",
    "        kwargs['scheduler'].step()\n",
    "    return train_loss/(i+1)\n",
    "\n",
    "# Input mean and std to normalize input.\n",
    "# Input subset to take a subset of coordinates.\n",
    "def train_model_with_generator(model, generator, optimizer, epochs, batch_size, iteration_per_epoch, loss_type='mse', q=0.5, val_data=None, **kwargs):\n",
    "    assert loss_type in ['mse', 'bce', 'quantile', 'max_quantile']\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for i in range(epochs):\n",
    "        train_loss = train_epoch_with_generator(\n",
    "            model, optimizer, generator, batch_size, iteration_per_epoch, loss_type, q, kwargs)\n",
    "        print('Epoch: {}'.format(i+1))\n",
    "        print('Train loss: {:.5f}'.format(train_loss))\n",
    "        train_losses.append(train_loss)\n",
    "        if val_data.__str__() != 'None':\n",
    "            val_loss = model_test(model, val_data, loss_type, q, kwargs)\n",
    "            print('Val loss: {:.5f}'.format(val_loss))\n",
    "            val_losses.append(val_loss)\n",
    "        if 'model_list' in kwargs:\n",
    "            if (i+1) in kwargs['save_point']:\n",
    "                kwargs['model_list'].append(deepcopy(model.state_dict()))\n",
    "        if 'coordinate_loss' in kwargs:\n",
    "            pred = predict(model, kwargs['Y_test'])\n",
    "            kwargs['coordinate_loss'].append(np.mean(np.maximum(q*(kwargs['beta_test']-pred),(1-q)*(pred-kwargs['beta_test'])), 0))\n",
    "        if 'save_paras' in kwargs:\n",
    "            for name, paras in model.named_parameters():\n",
    "                if name in kwargs['save_paras']:\n",
    "                    kwargs['paras_dict'][name].append(paras.detach().cpu().numpy())\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.float64(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
