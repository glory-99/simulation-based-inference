{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rng\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import math \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generators and model specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X\n",
    "X is generated and normalized as in low dimension case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 200 # number of dimension\n",
    "N = 50 # sample size\n",
    "rho = 0 # correlation between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('./data/X_rho0_N50_p200.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse linear model\n",
    "$\\gamma_j \\sim Bernoulli(\\theta)$ (We fix $\\theta=0.05$ currently, but we can also add a prior on $\\theta$ later.)\\\n",
    "$\\beta_j | \\gamma_j=1 \\sim U(-3,3), P(\\beta_j=0|\\gamma_j=0)=1$\\\n",
    "$Y=X\\beta+\\epsilon,\\epsilon_i\\sim N(0,\\sigma^2)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(object):\n",
    "    def __init__(self, p, theta, beta_range, N, X) -> None:\n",
    "        self.p = p \n",
    "        self.theta = theta \n",
    "        self.beta_range = beta_range \n",
    "        self.N = N\n",
    "        self.X = X\n",
    "    \n",
    "    def generate_samples(self, n, seed):\n",
    "        rng.seed(seed)\n",
    "        scale = self.beta_range[1] - self.beta_range[0]\n",
    "        theta = np.ones((n, self.p)) * self.theta\n",
    "        gamma = rng.binomial(1, theta)\n",
    "        beta = np.zeros((n, self.p))\n",
    "        beta[gamma == 1] = rng.rand(np.sum(gamma == 1)) * scale + self.beta_range[0]\n",
    "        beta[gamma == 0] = 0. \n",
    "        Y = beta@self.X.T + rng.randn(n, self.N)\n",
    "        return gamma, beta, Y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Four layers MLP\n",
    "* Four hidden layers, with 1024, 2048, 2048 and 1024 hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, N, p):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(N, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 2048)\n",
    "        self.fc3 = nn.Linear(2048, 2048)\n",
    "        self.fc4 = nn.Linear(2048, 1024)\n",
    "        self.fc5 = nn.Linear(1024, p)\n",
    "        self.mseloss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, input):\n",
    "        u = self.relu(self.fc1(input))\n",
    "        u = self.relu(self.fc2(u))\n",
    "        u = self.relu(self.fc3(u))\n",
    "        u = self.relu(self.fc4(u))\n",
    "        output = self.fc5(u)\n",
    "        return output\n",
    "\n",
    "    def get_mseloss(self, data, targ):\n",
    "        output = self.forward(data)\n",
    "        loss = self.mseloss(output, targ)\n",
    "        return loss \n",
    "    \n",
    "    def get_bceloss(self, data, targ):\n",
    "        output = self.forward(data)\n",
    "        loss = self.bceloss(output, targ)\n",
    "        return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    " loss_type: 'mse' for posterior mean, 'bce' for predicting whether beta is 0, 'quantile' for posterior quantile.\n",
    "q: Only used when loss_type is 'quantile', q quantile.\n",
    "'''\n",
    "def train_epoch(model, optimizer, train_data, train_labels, batch_size, loss_type, q):\n",
    "    model.train()\n",
    "    n = train_data.shape[0]\n",
    "    train_loss = 0.\n",
    "    for i in range(math.ceil(n/batch_size)):\n",
    "        data = torch.from_numpy(train_data[(i*batch_size):min((i+1)*batch_size, n-1)]).type(torch.float).to(device)\n",
    "        targ = torch.from_numpy(train_labels[(i*batch_size):min((i+1)*batch_size, n-1)]).type(torch.float).to(device)\n",
    "        if loss_type == 'mse':\n",
    "            loss = model.get_mseloss(data, targ)\n",
    "        elif loss_type == 'bce':\n",
    "            loss = model.get_bceloss(data, targ)\n",
    "        elif loss_type == 'quantile':\n",
    "            loss = model.get_quanloss(data, targ, q)\n",
    "        train_loss += loss.item() * data.shape[0]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return train_loss/n \n",
    "\n",
    "def model_test(model, test_data, test_labels, loss_type='mse', q=0.5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        data = torch.from_numpy(test_data).type(torch.float).to(device)\n",
    "        targ = torch.from_numpy(test_labels).type(torch.float).to(device)\n",
    "        if loss_type == 'mse':\n",
    "            loss = model.get_mseloss(data, targ)\n",
    "        elif loss_type == 'bce':\n",
    "            loss = model.get_bceloss(data, targ)\n",
    "        elif loss_type == 'quantile':\n",
    "            loss = model.get_quanloss(data, targ, q)\n",
    "    return loss.item()\n",
    "\n",
    "def train_model(model, lr, batch_size, epochs, train_data, train_labels, loss_type='mse', q=0.5, val_data=None, val_labels=None):\n",
    "    assert loss_type in ['mse', 'bce', 'quantile']\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for i in range(epochs):\n",
    "        train_loss = train_epoch(model, optimizer, train_data, train_labels, batch_size, loss_type, q)\n",
    "        print('Epoch: {}'.format(i+1))\n",
    "        print('Train loss: {:.5f}'.format(train_loss))\n",
    "        train_losses.append(train_loss)\n",
    "        if isinstance(val_data, np.ndarray):\n",
    "            val_loss = model_test(model, val_data, val_labels, loss_type, q)\n",
    "            print('Val loss: {:.5f}'.format(val_loss))\n",
    "            val_losses.append(val_loss)\n",
    "    return train_losses, val_losses\n",
    "\n",
    "def predict(model, Y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        data = torch.from_numpy(Y).type(torch.float).to(device)\n",
    "        pred = model(data)\n",
    "    return pred.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_val_rho0 = np.load()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "28f8cf0a8a4d3996c532b07e251eb4af62f7beb2b6ca98348312838517986615"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
